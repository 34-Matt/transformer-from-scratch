{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "The main area, containing the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import T1000\n",
    "importlib.reload(T1000)  # Force reload of T1000\n",
    "from T1000 import *\n",
    "from utility import get_gutenberg_book, get_many_books\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 84...\n",
      "\t419422 characters read\n",
      "Getting book 15...\n",
      "\t1218778 characters read\n",
      "Getting book 18...\n",
      "\t1172824 characters read\n",
      "Getting book 82...\n",
      "\t1103796 characters read\n",
      "Getting book 996...\n",
      "\t2299352 characters read\n",
      "Getting book 2600...\n",
      "\t3208337 characters read\n",
      "sum(len(x) for x in DATA_RAW) = 9422509\n",
      "Vocabulary size: 10000\n",
      "Max token value in int_sequences: 9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, [0, 1422, 1, 0, 0, 21, 339, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])\n",
    "print(f\"{sum(len(x) for x in DATA_RAW) = }\")\n",
    "\n",
    "def create_word_to_int_mapping(texts: list[str], max_vocab_size: int = 10000) -> tuple[dict[str, int], list[list[int]]]:\n",
    "    word_counts = {}\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:max_vocab_size - 1]\n",
    "    vocab_words = [word for word, _ in vocab]\n",
    "    \n",
    "    word_to_int = {\"<UNK>\": 0}\n",
    "    for i, word in enumerate(vocab_words, 1):\n",
    "        word_to_int[word] = i\n",
    "    \n",
    "    int_sequences = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        int_sequence = [word_to_int.get(word, 0) for word in words]\n",
    "        int_sequences.append(int_sequence)\n",
    "    \n",
    "    return word_to_int, int_sequences\n",
    "\n",
    "word_to_int, int_sequences = create_word_to_int_mapping(DATA_RAW, max_vocab_size=10000)\n",
    "max_token = max(max(seq) for seq in int_sequences if seq)\n",
    "print(f\"Vocabulary size: {len(word_to_int)}\")\n",
    "print(f\"Max token value in int_sequences: {max_token}\")\n",
    "if max_token >= 10000:\n",
    "    raise ValueError(f\"Max token {max_token} exceeds expected vocab size 10000\")\n",
    "word_to_int[\"the\"], int_sequences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences: list[list[int]], context_size: int):\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.context_size = context_size\n",
    "        for sequence in sequences:\n",
    "            if len(sequence) <= context_size:\n",
    "                continue\n",
    "            for i in range(len(sequence) - context_size):\n",
    "                self.data.append(sequence[i:i + context_size])\n",
    "                self.targets.append(sequence[i + 1:i + context_size + 1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data[idx], dtype=torch.long),\n",
    "                torch.tensor(self.targets[idx], dtype=torch.long))\n",
    "\n",
    "def train_transformer(\n",
    "    model: Transformer,\n",
    "    int_sequences: list[list[int]],\n",
    "    context_size: int = 10,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 5,\n",
    "    learning_rate: float = 0.001,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    dataset = TextDataset(int_sequences, context_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"Training on {len(dataloader)} batches per epoch\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # inputs: [32, 10], targets: [32, 10]\n",
    "            optimizer.zero_grad()  # Reset gradients for the batch\n",
    "            \n",
    "            # Process each sequence in the batch individually\n",
    "            batch_loss = 0\n",
    "            for i in range(inputs.shape[0]):  # Loop over batch_size (32)\n",
    "                single_input = inputs[i].to(device)   # [10]\n",
    "                single_target = targets[i].to(device) # [10]\n",
    "                single_output = model(single_input)   # [10, 10000]\n",
    "                loss = criterion(single_output, single_target)\n",
    "                loss.backward()  # Accumulate gradients\n",
    "                batch_loss += loss.item()\n",
    "            \n",
    "            # Clip gradients and update weights once per batch\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss / batch_size  # Average loss over batch\n",
    "            num_batches += 1\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, Avg Loss: {batch_loss / batch_size:.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Initialize and train\n",
    "config = GPTConfig()\n",
    "model = Transformer(config)\n",
    "print(f\"Model vocab size: {model.embedding.num_embeddings}\")\n",
    "train_transformer(model, int_sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
